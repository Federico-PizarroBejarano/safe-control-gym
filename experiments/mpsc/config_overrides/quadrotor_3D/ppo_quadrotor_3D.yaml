algo: ppo
algo_config:
  # model args
  hidden_dim: 128

  # loss args
  use_gae: True
  entropy_coef: 0.01

  # optim args
  opt_epochs: 20
  mini_batch_size: 256
  actor_lr: 0.001
  critic_lr: 0.001

  # runner args
  max_env_steps: 1000000
  rollout_batch_size: 1
  rollout_steps: 1000
  eval_batch_size: 25

  # misc
  log_interval: 10000
  save_interval: 0
  num_checkpoints: 0
  eval_interval: 10000
  eval_save_best: True
  tensorboard: False

  # safety filter
  filter_train_actions: True
  penalize_sf_diff: True
  sf_penalty: 300
  use_safe_reset: True
